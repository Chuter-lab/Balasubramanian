{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Madhusudhanan Balasubramanian (MB), Ph.D., The University of Memphis\n",
    "#### Visualize model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Libraries / modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Import Mask RCNN\n",
    "ROOT_DIR = \"./Mask_RCNN\"\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "#\n",
    "from keras.preprocessing.image import img_to_array\n",
    "#\n",
    "\n",
    "#Using pycocotools for AP and AR calculations as in centermask\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "import pycocotools.mask as mask_util\n",
    "import axon_coco as coco #copied samples/coco/coco.py as axon_coco.py\n",
    "from axonlib.cocoeval import COCOeval\n",
    "import pandas as pd\n",
    "#\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "\n",
    "#Visualization\n",
    "from axonlib.axon_visualizer_mrcnn import SimpleMetadata, Visualizer, ColorMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceParams():\n",
    "    def __init__(self, model_root_dir, model_subdir, model_weight_file,\n",
    "                 data_root_dir, dataset_name,\n",
    "                 mrcnn_lib_dir=None):\n",
    "        \n",
    "        debug_flag = 0\n",
    "\n",
    "        # Root directory of the project\n",
    "        if mrcnn_lib_dir is None:\n",
    "            self.mrcnn_lib_dir = \"./Mask_RCNN\"\n",
    "        else:\n",
    "            self.mrcnn_lib_dir = mrcnn_lib_dir\n",
    "\n",
    "        #Model configuration\n",
    "        self.inference_config = InferenceConfig()\n",
    "        if debug_flag == 1:\n",
    "            self.inference_config.display()\n",
    "\n",
    "        #Model params\n",
    "        self.model_root_dir = model_root_dir\n",
    "        self.model_path = os.path.join(model_root_dir, model_subdir)\n",
    "        self.model_weight_file = model_weight_file\n",
    "        self.model_abs_file = os.path.join(self.model_path, self.model_weight_file)\n",
    "\n",
    "        #Data params and initialization\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_dir = os.path.join(self.data_root_dir, self.dataset_name)\n",
    "        #\n",
    "        self.axon_dataset = coco.CocoDataset()\n",
    "        print(f\"self.data_root_dir: {self.data_root_dir}\")\n",
    "        self.input_coco_obj = self.axon_dataset.load_coco(self.data_root_dir, self.dataset_name, return_coco=True)\n",
    "        self.axon_dataset.prepare() # Must call before using the dataset\n",
    "\n",
    "        #Evaluation / output directory\n",
    "        self.eval_dir = os.path.join(self.model_path, 'Evaluations', dataset_name)\n",
    "        os.makedirs(self.eval_dir, exist_ok=True)\n",
    "        \n",
    "        ft_json_file = f\"{dataset_name}.json\"\n",
    "        self.gt_json_file = os.path.join(self.data_root_dir, self.dataset_name, ft_json_file)\n",
    "        self.predictions_file = os.path.join(self.eval_dir, \"coco_instances_results.json\")\n",
    "\n",
    "\n",
    "#Model configuration for inference\n",
    "class InferenceConfig(coco.CocoConfig):\n",
    "    #Dec 09, 2021 MB notes: Initially, no other configuration changes needed for training (recall / see that\n",
    "    # configuration changes required for inferences such as setting # GPUs to 1, etc. See axon_coco.py for \n",
    "    # other possible configuration changes\n",
    "    \n",
    "    #General model parameters\n",
    "    BACKBONE = 'resnet50' #default is resnet101\n",
    "    USE_MINI_MASK=True\n",
    "    MEAN_PIXEL = [123.7, 116.8, 103.9]\n",
    "        \n",
    "    #General training parameters\n",
    "    #----------------\n",
    "    IMAGES_PER_GPU = 1 #should be 1 for inference\n",
    "    GPU_COUNT = 1\n",
    "    BATCH_SIZE = IMAGES_PER_GPU * GPU_COUNT #BATCH_SIZE calculated only in config.py's constructor in line 216\n",
    "    #\n",
    "    STEPS_PER_EPOCH = 3 #Jan 07: reduced from 100 to 75; May 10, 2022: steps/epoch = #training images/batch size\n",
    "    VALIDATION_STEPS = 3 #originally 5, previously set at 15\n",
    "    #\n",
    "    GRADIENT_CLIP_NORM = 10.0\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "    WEIGHT_DECAY=0.0001\n",
    "    LOSS_WEIGHTS = { \"rpn_class_loss\": 1., \"rpn_bbox_loss\": 1., \"mrcnn_class_loss\": 1., \"mrcnn_bbox_loss\": 1., \"mrcnn_mask_loss\": 1. }\n",
    "\n",
    "    #RPN parameters\n",
    "    #---------------\n",
    "    # Length of square anchor side in pixels\n",
    "    #RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128) #Was used in V11 which generated the model e608\n",
    "    #RPN_ANCHOR_SCALES = (4, 8, 16, 32, 64) #Incorrectly set this in V11_Phase2 - that's why the model had issues\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    #RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "    RPN_ANCHOR_RATIOS = [0.25, 0.5, 1, 2, 4]\n",
    "    #References for increasing number of detections: https://github.com/matterport/Mask_RCNN/issues/1884#:~:text=What%20seems%20to%20have%20had%20the%20greatest%20impact%20for%20us%20were%20the%20training%20configs%3A\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 600 #default: 256; Number of anchors per image selected to train the RPN\n",
    "    MAX_GT_INSTANCES = 600 #Number of GT instances per image kept to train the network\n",
    "    \n",
    "    #Proposal layer parameters (not trainable layer; just filtering)\n",
    "    #-------------------------\n",
    "    PRE_NMS_LIMIT = 6000 #default: ;Number of anchors with the best RPN score that are retained\n",
    "    RPN_NMS_THRESHOLD = 0.9 #0.7 #default is 0.7; area overlap among candidate anchors before dropping the anchor with the lowest score; higher values increases the number of region proposals\n",
    "    POST_NMS_ROIS_TRAINING = 1800 # >Training; ROIs kept after non-maximum supression in the proposal layer; default is 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 8000 # >Inference; ROIs kept after NMS in the proposal layer based on their RPN score\n",
    "    \n",
    "    #Detection target layer parameters (training only; not a trainable layer; just filtering)\n",
    "    #Receives at most POST_NMS_ROIS_TRAINING number of anchors from the proposal layer\n",
    "    #Anchors whose IoU > 0.5 over ground truth are selected (at most POST_NMS_ROIS_TRAINING)\n",
    "    #---------------------------------\n",
    "    TRAIN_ROIS_PER_IMAGE = 600 #default is 200; no. of ROIs randomly selected out of POST_NMS_ROIS_TRAINING\n",
    "    ROI_POSITIVE_RATIO = 0.33 #default is 0.33; usually set as 1/#classes; i.e. 33% of TRAIN_ROIS_PER_IMAGE should be positive\n",
    "       \n",
    "    #Detection layer parameters (inference only; not trainable, just filtering)\n",
    "    #Receives at most POST_NMS_ROIS_INFERENCE number of anchors from the proposal layer\n",
    "    #MB: https://medium.com/@umdfirecoml/training-a-mask-r-cnn-model-using-the-nucleus-data-bcb5fdbc0181 \n",
    "    #----------------------------\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7 #default: 0.7; AOIs with lower confidence than this are discarded\n",
    "    DETECTION_NMS_THRESHOLD = 0.3 #reference: 0.3; AOIs those with higher overlapping areas are discarded based on their RPN score\n",
    "\n",
    "    #Feature Pyramid Network (FPN)\n",
    "    #Has a classifier and mask graph; identifies class and generates mask\n",
    "    #---------------------------------\n",
    "    DETECTION_MAX_INSTANCES = 600 # >Inference; maximum number of instances identified by Mask RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_GT_annotations(meta_data, image_mat, data_dict, output_image_file):\n",
    "    \"\"\"\n",
    "        Save visualization of GT object annotations\n",
    "    \"\"\"\n",
    "    \n",
    "    gt_outline_linewidth=0.8\n",
    "    gt_outline_linestyle='-'\n",
    "\n",
    "    #Instantiate Visualizer object \"v\" with the image and the metadata\n",
    "    viz_obj = Visualizer(image_mat[:, :, ::-1],\n",
    "                   metadata = meta_data, \n",
    "                   #MB: March 31, 2024\n",
    "                   #instance_mode=ColorMode.IMAGE_BW,   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "                   instance_mode=ColorMode.SEGMENTATION\n",
    "    )\n",
    "    #\n",
    "    #Draw the ground truth first\n",
    "    visImage_obj = viz_obj.draw_dataset_dict(data_dict, jittering=False, outline_linewidth=gt_outline_linewidth, outline_linestyle=gt_outline_linestyle)\n",
    "    output_image = visImage_obj.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(output_image_file, output_image[:, :, ::-1])\n",
    "    #\n",
    "    #visImage_obj.save(output_image_file)\n",
    "\n",
    "def save_detected_annotations(meta_data, image_mat, detected_instances, output_image_file):\n",
    "    \"\"\"\n",
    "        Save visualization of annotations of objects detected by the model\n",
    "    \"\"\"\n",
    "    \n",
    "    dt_outline_linewidth = 0.8\n",
    "    dt_outline_linestyle = '-'\n",
    "\n",
    "    #Instantiate Visualizer object \"v\" with the image and the metadata\n",
    "    viz_obj = Visualizer(image_mat[:, :, ::-1],\n",
    "                   metadata = meta_data, \n",
    "                   #MB: March 31, 2024\n",
    "                   #instance_mode=ColorMode.IMAGE_BW,   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "                   instance_mode=ColorMode.SEGMENTATION\n",
    "    )\n",
    "    #\n",
    "    #Draw the detected annotations on top of the input image\n",
    "    visImage_obj = viz_obj.draw_instance_predictions(detected_instances, jittering=False, outline_linewidth=dt_outline_linewidth)\n",
    "    output_image = visImage_obj.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(output_image_file, output_image)\n",
    "    #\n",
    "    #visImage_obj.save(output_image_file)\n",
    "\n",
    "def save_GT_DT_combined_annotations(meta_data, image_mat, data_dict, detected_instances, output_image_file):\n",
    "    \"\"\"\n",
    "        Save combined annotation of GT objects and objects detected by the model\n",
    "    \"\"\"\n",
    "\n",
    "    gt_outline_linewidth = 0.8\n",
    "    gt_outline_linestyle = ':'\n",
    "    dt_outline_linewidth = 0.4\n",
    "\n",
    "    #GT image\n",
    "    #\n",
    "    #Instantiate Visualizer object \"v\" with the image and the metadata\n",
    "    viz_obj = Visualizer(image_mat[:, :, ::-1],\n",
    "                   metadata = meta_data, \n",
    "                   #MB: March 31, 2024\n",
    "                   #instance_mode=ColorMode.IMAGE_BW,   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "                   instance_mode=ColorMode.SEGMENTATION\n",
    "    )\n",
    "    #\n",
    "    #Draw the ground truth first\n",
    "    visImage_obj = viz_obj.draw_dataset_dict(data_dict, jittering=False, outline_linewidth=gt_outline_linewidth, outline_linestyle=gt_outline_linestyle)\n",
    "\n",
    "    #Draw model detections on top of the ground truth\n",
    "    visImage_obj = viz_obj.draw_instance_predictions(detected_instances, jittering=False, outline_linewidth=dt_outline_linewidth)\n",
    "\n",
    "    # Get the drawing as image\n",
    "    out_image_combined = visImage_obj.get_image()[:, :, ::-1]\n",
    "    cv2.imwrite(output_image_file, out_image_combined[:, :, ::-1])\n",
    "    #\n",
    "    #visImage_obj.save(output_image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_instance_predictions_image(inf_params):\n",
    "    \n",
    "    #Initialize\n",
    "    input_coco_obj = inf_params.input_coco_obj\n",
    "    axon_dataset = inf_params.axon_dataset\n",
    "    curr_meta_data = SimpleMetadata(thing_classes=['Necrotic', 'Healthy'], \n",
    "                                    thing_colors = [(0, 0, 255), (255, 0, 0), (0, 255, 0)])\n",
    "\n",
    "    #Create a Mask RCNN model in \"inference\" mode\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                              config=inf_params.inference_config, \n",
    "                              model_dir = inf_params.model_root_dir)\n",
    "\n",
    "    #Load model file\n",
    "    model.load_weights(inf_params.model_abs_file, by_name=True)\n",
    "\n",
    "    image_ids = []\n",
    "    all_results = []\n",
    "    category_ids = input_coco_obj.getCatIds()\n",
    "    #print(f\"category_ids: {category_ids}\")\n",
    "    #print(f\"image ids: {axon_dataset.image_ids}\")\n",
    "    #print(f\"image ids: {input_coco_obj.getImgIds()}\")\n",
    "    all_image_ids = input_coco_obj.getImgIds()\n",
    "    for image_ind, image_id in enumerate(all_image_ids):\n",
    "        \n",
    "        #MB, important: load_image_gt is scaling the image to 1024 x 1024 before feeding to detect().\n",
    "        # But detect() has internal mechanisms to scale/mold and unscale/unmold.  This is done based on the \n",
    "        # size of the input image and the config parameters IMAGE_MIN_SIZE, IMAGE_MAX_SIZE.  With load_image_gt(),\n",
    "        # and with IMAGE_MAX_SIZE = 1024, the output from detect() is not unmolded.  i.e. detections won't match\n",
    "        # with the original gt geometry of the annotations - replacing with simple imread instead\n",
    "        #\n",
    "        #original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        #    modellib.load_image_gt(axon_dataset, inf_params.inference_config, image_ind, use_mini_mask=False)\n",
    "        #MB, problem reading from axon_dataset.image_info, use coco obj instead\n",
    "        #print(f\"image_id:{image_id}; ind: {image_ind}; path:{axon_dataset.image_info[image_ind]['path']}\")\n",
    "        #original_image = cv2.imread(axon_dataset.image_info[image_ind]['path'])\n",
    "        img_file_name = os.path.join(inf_params.data_dir, input_coco_obj.imgs[image_id]['file_name'])\n",
    "        print(f\"image_id:{image_id}; ind: {image_ind}; img_file_name: {img_file_name}\")\n",
    "        original_image = cv2.imread(img_file_name)\n",
    "        \n",
    "        #MB note: it appears mold_image normalizes the image (subtract dataset mean)\n",
    "        #         img_to_array(image) is used in test_axon_model_v1.ipynb\n",
    "        #         This needs to be verified:\n",
    "        #           detect() molds all input images in a format for feeding to the network\n",
    "        #           So, no need to mold before calling detect\n",
    "        #           expand_dims adds a new axis to represent multiple images fed to the network for detection\n",
    "        #           np.expand_dims(image, 0) is same as [image]--the latter is a list with single entry\n",
    "        #scaled_image = modellib.mold_image(image, cfg)\n",
    "        #image_array = np.expand_dims(scaled_image, 0)\n",
    "        image_array = img_to_array(original_image)\n",
    "        #print(f\"image_array.shape: {image_array.shape}; dtype: {image_array.dtype}; max: {image_array.max()}; min: {image_array.min()}\")\n",
    "        #plt.imshow(image_array.astype(np.uint8))\n",
    "        #plt.show()\n",
    "\n",
    "        #Detect objects in the image\n",
    "        result = model.detect([image_array])[0]\n",
    "        #\n",
    "        #boxes = result[\"rois\"]\n",
    "        #masks = result[\"masks\"]\n",
    "        #class_ids = result[\"class_ids\"]\n",
    "        #class_names = axon_dataset.class_names\n",
    "        #scores = result[\"scores\"]\n",
    "\n",
    "        DT_image_file = os.path.join(inf_params.eval_dir, input_coco_obj.imgs[image_id]['file_name'])\n",
    "        save_detected_annotations(curr_meta_data, image_array, result, DT_image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root_dir = \"./logs/\"\n",
    "model_subdir = \"coco20250418T0934\"\n",
    "model_weight_file = \"mask_rcnn_coco_0263.h5\"\n",
    "#\n",
    "# Data directory\n",
    "data_root_dir = \"./DataFiles/Phase3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.data_root_dir: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/madhu/Lab/Members/00_madhu/Programs/axon_segmentation/Mask_RCNN/mrcnn/model.py:347: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/madhu/Lab/Members/00_madhu/Programs/axon_segmentation/Mask_RCNN/mrcnn/model.py:405: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/madhu/Lab/Members/00_madhu/Programs/axon_segmentation/Mask_RCNN/mrcnn/model.py:429: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /home/madhu/Lab/Members/00_madhu/Programs/axon_segmentation/Mask_RCNN/mrcnn/model.py:726: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/madhu/Lab/Members/00_madhu/Programs/axon_segmentation/Mask_RCNN/mrcnn/model.py:728: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/madhu/Lab/Members/00_madhu/Programs/axon_segmentation/Mask_RCNN/mrcnn/model.py:778: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/venv_mamba/envs/venv_maskrcnn/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Re-starting from epoch 263\n",
      "image_id:5167; ind: 0; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/2006---Rat-77E6772---p88-F_m10_06_A_D.tif\n",
      "image_id:5168; ind: 1; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/2006---Rat-77E7654---p87-M--NEW_m10_11_D_A.tif\n",
      "image_id:5169; ind: 2; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/2006---Rat-77E7654---p87-M--NEW_m16_07_B_C.tif\n",
      "image_id:5170; ind: 3; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/2006---Rat-77E82F7---p92-M---NEW_m03_14_C_D.tif\n",
      "image_id:5171; ind: 4; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/2006---Rat-77E82F7---p92-M---NEW_m04_11_D_A.tif\n",
      "image_id:5172; ind: 5; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78A16DB-84-1903-12_m02_13_C_C.tif\n",
      "image_id:5173; ind: 6; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78A1A9A-69-1903-15_m03_14_C_D.tif\n",
      "image_id:5174; ind: 7; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78A1A9A-69-1903-15_m07_06_A_D.tif\n",
      "image_id:5175; ind: 8; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78A2547-59-1903-12_m02_15_D_C.tif\n",
      "image_id:5176; ind: 9; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78A2547-59-1903-12_m02_16_D_D.tif\n",
      "image_id:5177; ind: 10; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78A2547-59-1903-12_m04_09_C_A.tif\n",
      "image_id:5178; ind: 11; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78AO1BF-60-1903-15_m02_10_C_B.tif\n",
      "image_id:5179; ind: 12; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78AO1BF-60-1903-15_m02_11_D_A.tif\n",
      "image_id:5180; ind: 13; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Validation/Rat-78AO1BF-60-1903-15_m02_12_D_B.tif\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Phase3_Validation\"\n",
    "bbox_or_segm = \"segm\"\n",
    "\n",
    "inf_params = InferenceParams(model_root_dir, model_subdir, model_weight_file,\n",
    "                             data_root_dir, dataset_name, mrcnn_lib_dir=None)\n",
    "\n",
    "#Evaluate the model with the given dataset in dataset_name\n",
    "save_instance_predictions_image(inf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.data_root_dir: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Re-starting from epoch 263\n",
      "image_id:5181; ind: 0; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/2006---Rat-77E6772---p88-F_m15_09_C_A.tif\n",
      "image_id:5182; ind: 1; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/2006---Rat-77E6772---p88-F_m16_05_A_C.tif\n",
      "image_id:5183; ind: 2; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/2006---Rat-77E7654---p87-M--NEW_m19_06_A_D.tif\n",
      "image_id:5184; ind: 3; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/2006---Rat-77E7654---p87-M--NEW_m23_08_B_D.tif\n",
      "image_id:5185; ind: 4; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/2006---Rat-77E7975---p86-F--NEW_m02_13_C_C.tif\n",
      "image_id:5186; ind: 5; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/2006---Rat-77E82F7---p92-M---NEW_m22_05_A_C.tif\n",
      "image_id:5187; ind: 6; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/Rat-78A16DB-84-1903-12_m02_15_D_C.tif\n",
      "image_id:5188; ind: 7; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/Rat-78A16DB-84-1903-12_m03_15_D_C.tif\n",
      "image_id:5189; ind: 8; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/Rat-78A1A9A-69-1903-15_m10_09_C_A.tif\n",
      "image_id:5190; ind: 9; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/Rat-78A2547-59-1903-12_m04_13_C_C.tif\n",
      "image_id:5191; ind: 10; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/Rat-78A2547-59-1903-12_m07_14_C_D.tif\n",
      "image_id:5192; ind: 11; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/Rat-78AO1BF-60-1903-15_m02_13_C_C.tif\n",
      "image_id:5193; ind: 12; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Phase3_Testing/Rat-78AO1BF-60-1903-15_m02_14_C_D.tif\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Phase3_Testing\"\n",
    "bbox_or_segm = \"segm\"\n",
    "\n",
    "inf_params = InferenceParams(model_root_dir, model_subdir, model_weight_file,\n",
    "                             data_root_dir, dataset_name, mrcnn_lib_dir=None)\n",
    "\n",
    "#Evaluate the model with the given dataset in dataset_name\n",
    "save_instance_predictions_image(inf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.data_root_dir: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3\n",
      "loading annotations into memory...\n",
      "Done (t=0.31s)\n",
      "creating index...\n",
      "index created!\n",
      "Re-starting from epoch 263\n",
      "image_id:5254; ind: 0; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON01_1x1.tif\n",
      "image_id:5255; ind: 1; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON01_1x2.tif\n",
      "image_id:5256; ind: 2; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON01_2x1.tif\n",
      "image_id:5257; ind: 3; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON01_2x2.tif\n",
      "image_id:5258; ind: 4; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON02_1x1.tif\n",
      "image_id:5259; ind: 5; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON02_1x2.tif\n",
      "image_id:5260; ind: 6; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON02_2x1.tif\n",
      "image_id:5261; ind: 7; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON02_2x2.tif\n",
      "image_id:5262; ind: 8; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON03_1x1.tif\n",
      "image_id:5263; ind: 9; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON03_1x2.tif\n",
      "image_id:5264; ind: 10; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON03_2x1.tif\n",
      "image_id:5265; ind: 11; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON03_2x2.tif\n",
      "image_id:5266; ind: 12; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON04_1x1.tif\n",
      "image_id:5267; ind: 13; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON04_1x2.tif\n",
      "image_id:5268; ind: 14; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON04_2x1.tif\n",
      "image_id:5269; ind: 15; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON04_2x2.tif\n",
      "image_id:5270; ind: 16; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON05_1x1.tif\n",
      "image_id:5271; ind: 17; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON05_1x2.tif\n",
      "image_id:5272; ind: 18; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON05_2x1.tif\n",
      "image_id:5273; ind: 19; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6770ON05_2x2.tif\n",
      "image_id:5274; ind: 20; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON01_1x1.tif\n",
      "image_id:5275; ind: 21; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON01_1x2.tif\n",
      "image_id:5276; ind: 22; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON01_2x1.tif\n",
      "image_id:5277; ind: 23; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON01_2x2.tif\n",
      "image_id:5278; ind: 24; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON02_1x1.tif\n",
      "image_id:5279; ind: 25; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON02_1x2.tif\n",
      "image_id:5280; ind: 26; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON02_2x1.tif\n",
      "image_id:5281; ind: 27; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON02_2x2.tif\n",
      "image_id:5282; ind: 28; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON05_1x1.tif\n",
      "image_id:5283; ind: 29; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON05_1x2.tif\n",
      "image_id:5284; ind: 30; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON05_2x1.tif\n",
      "image_id:5285; ind: 31; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6772ON05_2x2.tif\n",
      "image_id:5286; ind: 32; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON01_1x1.tif\n",
      "image_id:5287; ind: 33; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON01_1x2.tif\n",
      "image_id:5288; ind: 34; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON01_2x1.tif\n",
      "image_id:5289; ind: 35; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON01_2x2.tif\n",
      "image_id:5290; ind: 36; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON02_1x1.tif\n",
      "image_id:5291; ind: 37; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON02_1x2.tif\n",
      "image_id:5292; ind: 38; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON02_2x1.tif\n",
      "image_id:5293; ind: 39; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON02_2x2.tif\n",
      "image_id:5294; ind: 40; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON03_1x1.tif\n",
      "image_id:5295; ind: 41; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON03_1x2.tif\n",
      "image_id:5296; ind: 42; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON03_2x1.tif\n",
      "image_id:5297; ind: 43; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON03_2x2.tif\n",
      "image_id:5298; ind: 44; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON04_1x1.tif\n",
      "image_id:5299; ind: 45; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON04_1x2.tif\n",
      "image_id:5300; ind: 46; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON04_2x1.tif\n",
      "image_id:5301; ind: 47; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON04_2x2.tif\n",
      "image_id:5302; ind: 48; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON05_1x1.tif\n",
      "image_id:5303; ind: 49; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON05_1x2.tif\n",
      "image_id:5304; ind: 50; img_file_name: /home/madhu/Lab/Members/00_madhu/DataFiles/Phase3/Quigley_Eval/Ms6784ON05_2x1.tif\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"Quigley_Eval\"\n",
    "bbox_or_segm = \"segm\"\n",
    "\n",
    "inf_params = InferenceParams(model_root_dir, model_subdir, model_weight_file,\n",
    "                             data_root_dir, dataset_name, mrcnn_lib_dir=None)\n",
    "\n",
    "#Evaluate the model with the given dataset in dataset_name\n",
    "save_instance_predictions_image(inf_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
